#!/usr/bin/env python3
from pathlib import Path 
from importlib.util import spec_from_file_location, module_from_spec
from perfins.PIFGfxReader import *

nvperf_path = Path(__file__).with_name("nvperf")
nvperf_spec = spec_from_file_location("nvperf", nvperf_path)
nvperf = module_from_spec(nvperf_spec)
nvperf_spec.loader.exec_module(nvperf)

with nvperf.Timer():
    spark_pi_reader = PIFGfxReader(r"C:\Users\WanliZhu\Downloads\PICapture_HW_CHI_on_spark\.pfm")
    spark_frames  = spark_pi_reader.getSpanValues("frame_0")
    impact1_total_slowdown = 0
    impact1_chi_caused_slowdown = 0
    impact2_chi_caused_slowdown = 0

    def get_spark_metrics_with_range(start, end):
        return spark_pi_reader.getMetricData(
            [
                '06_MMU_limited',
                '07_RASTER_limited',
                '08_Rop_limited',
                '10_GCC_limited',
                '11_LTC_limited',
                '12_LST_limited',
                '13_TTU_limited',
                '14_SM_limited',
                '16_Launch_pipe_limited',
                '17_HSHUB_limited',
                'DRAM_limited',
                'HSHUB_limited',
                'LTC_limited',
                'HSHUB___DRAM_GB_per_s', 
                'HSHUB___read_bytes',
                'HSHUB___write_bytes',
                'ltc_chi_cmd_out_ReadNoSnp_fbp{FBP}_ltc{LTC}_l2slice{L2SLICE}',
                'ltc_chi_cmd_out_WriteNoSnpPartial_fbp{FBP}_ltc{LTC}_l2slice{L2SLICE}',
                'ltc_chi_cmd_out_WriteNoSnpFull_fbp{FBP}_ltc{LTC}_l2slice{L2SLICE}'
            ],
            timeStart=start,
            timeEnd=end 
        )

    def is_spark_HW_CHI_limited(spark_bucket_metrics, solution):
        def avg(samples):
            return sum(samples) / len(samples)

        is_dram_limited = True
        for limiter in [x for x in spark_bucket_metrics if "_limited" in x]:
            if avg(spark_bucket_metrics[limiter]) > avg(spark_bucket_metrics["DRAM_limited"]):
                is_dram_limited = False
        
        hshub_rw_bytes = avg(spark_bucket_metrics["HSHUB___read_bytes"]) + avg(spark_bucket_metrics["HSHUB___write_bytes"])
        chi_cmd_count = avg(spark_bucket_metrics["ltc_chi_cmd_out_ReadNoSnp_fbp{FBP}_ltc{LTC}_l2slice{L2SLICE}"]) + \
            avg(spark_bucket_metrics["ltc_chi_cmd_out_WriteNoSnpPartial_fbp{FBP}_ltc{LTC}_l2slice{L2SLICE}"]) + \
            avg(spark_bucket_metrics["ltc_chi_cmd_out_WriteNoSnpFull_fbp{FBP}_ltc{LTC}_l2slice{L2SLICE}"])
        hshub_chicmd_ratio = 0 if chi_cmd_count == 0 else (hshub_rw_bytes / chi_cmd_count)
        
        is_single_sector_io = hshub_chicmd_ratio >= 30 and hshub_chicmd_ratio <= 50 
        is_dram_write_dominated = avg(spark_bucket_metrics['HSHUB___write_bytes']) > avg(spark_bucket_metrics['HSHUB___read_bytes'])
        
        if solution == 1:
            return is_dram_limited or is_single_sector_io or is_dram_write_dominated
        if solution == 2:
            return is_dram_limited and (is_single_sector_io or is_dram_write_dominated)
        
    # Traverse aligned spark and proxy frames 
    for i in range(len(spark_frames)):
        spark_frame = {
            "start": int(spark_frames[i]["start"]), 
            "end": int(spark_frames[i]["end"]),
            "duration": int(spark_frames[i]["end"]) - int(spark_frames[i]["start"])
        }
        spark_frame_buckets = spark_pi_reader.getSpanValues(name="3D0_0", 
                                                            timeStart=spark_frame["start"],
                                                            timeEnd=spark_frame["end"])
        if not spark_frame_buckets:
            continue 

        max_frame_bucket_pos = len(spark_frame_buckets)
        frame_time_diff = spark_frame["duration"]

        for j in range(max_frame_bucket_pos):
            spark_bucket = {
                "start": int(spark_frame_buckets[j]["start"]),
                "end": int(spark_frame_buckets[j]["end"]), 
                "duration": int(spark_frame_buckets[j]["end"]) - int(spark_frame_buckets[j]["start"])
            }
            spark_bucket_metrics = get_spark_metrics_with_range(start=spark_bucket["start"], end=spark_bucket["end"])
            if not spark_bucket_metrics:
                continue 

            bucket_time_diff = spark_bucket["duration"]
            impact1_total_slowdown += bucket_time_diff
            if is_spark_HW_CHI_limited(spark_bucket_metrics, 1):
                impact1_chi_caused_slowdown += bucket_time_diff
            if is_spark_HW_CHI_limited(spark_bucket_metrics, 2):
                impact2_chi_caused_slowdown += bucket_time_diff

    impact1 = 1.0 * impact1_chi_caused_slowdown / impact1_total_slowdown
    impact2 = 1.0 * impact2_chi_caused_slowdown / impact1_total_slowdown
    print(f"HW CHI limiter impact on spark")
    print(f"  - Formula 1: {impact1:.2%}")
    print(f"  - Formula 2: {impact2:.2%}")
